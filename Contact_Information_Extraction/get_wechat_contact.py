from simpletransformers.classification import ClassificationModel, ClassificationArgs
import numpy as np
import pandas as pd
from scipy.special import softmax
import csv
from simpletransformers.ner import NERModel, NERArgs
import jieba
import logging
from sklearn.metrics import f1_score,  accuracy_score
from sklearn.model_selection import train_test_split
import torch
import re

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

NER_model_path = '/data/jlxue/RBSEO_wechat_NER_train_model'
classify_model_path = '/data/jlxue/RBSEO_contact_classifier_train_model'

terms_with_wechat = set()
terms_extracted = set()

def numeric_num(term):
    cnt = 0
    for s in term:
        if s.isdecimal():
            cnt += 1
    return cnt


def postprocess_wechat(im:str):
    im = im.lower()
    im_split = im.split('-')
    im_final = ''
    for item in im_split:
        if numeric_num(item) >= 7:
            im_final = item
            break
        else:
            im_final += item
    im_final = re.sub(r'[^a-zA-Z0-9_]', '', im_final)
    im_final = im_final.replace('vx', '')
    im_final = im_final.replace('wechat', '')
    if numeric_num(im_final) >= 7:
        im_final = re.sub(r'[a-zA-Z_]', '', im_final)
    return im_final

def get_wechat_contact(terms_all_list, wechat_contact):
    unicode_similar_nums = [ 
        '0ÎŸÎ¿ÏƒÐžÐ¾ï¼Õ•Ö…×¡Ù‡Ù¥Ú¾â“¿ÛÛ•Ûµß€à¥¦à§¦à©¦à«¦à¬ à­¦à¯¦à°‚à±¦à²‚à³¦à´‚à´ àµ¦à¶‚à¹à»á€á€áƒ¿á‹á´á´‘â„´â²žâ²Ÿâµ”ã€‡ê“³ê¬½ï®¦ï®§ï®¨ï®©ï®ªï®«ï®¬ï®­ï»©ï»ªï»«ï»¬ï¼ï¼¯ï½ðŠ’ðŠ«ð„ð¬ð“‚ð“ªð”–ð‘“ð‘¢µð‘£ˆð‘£—ð‘£ ðŽð¨ð‘‚ð‘œð‘¶ð’ð’ªð“žð“¸ð”’ð”¬ð•†ð• ð•ºð–”ð–®ð—ˆð—¢ð—¼ð˜–ð˜°ð™Šð™¤ð™¾ðš˜ðš¶ð›ð›”ð›°ðœŠðœŽðœªð„ðˆð¤ð¾ðž‚ðžžðž¸ðž¼ðŸŽðŸ˜ðŸ¢ðŸ¬ðŸ¶ðž¸¤ðž¹¤ðžº„ðŸ¯°',
        '1|Ä±â’ˆÆ–â‘ â¶ï¼‘Ç€âžŠÉ©ÉªË›ÍºÎ™Î¹Ð†Ñ–Ó€Ó×€×•×ŸØ§Ù¡Û±ßŠáŽ¥á›á¾¾â„â„‘â„“â„¹â…ˆâ… â…°â…¼âˆ£â³â½â²’âµê“²ê™‡ê­µïºïºŽï¼‘ï¼©ï½‰ï½Œï¿¨ðŠŠðŒ‰ðŒ ð‘£ƒð–¼¨ðˆð¢ð¥ð¼ð‘–ð‘™ð‘°ð’Šð’ð’¾ð“ð“˜ð“²ð“µð”¦ð”©ð•€ð•šð•ð•´ð–Žð–‘ð–¨ð—‚ð—…ð—œð—¶ð—¹ð˜ð˜ªð˜­ð™„ð™žð™¡ð™¸ðš’ðš•ðš¤ðš°ð›Šð›ªðœ„ðœ¤ðœ¾ðžð¸ðž˜ðž²ðŸðŸ™ðŸ£ðŸ­ðŸ·ðž£‡ðž¸€ðžº€ðŸ¯±',
        '2Æ§Ï¨â’‰á’¿ê™„â‘¡ê›¯â·êšâž‹ï¼’ðŸðŸšðŸ¤ðŸ®ðŸ¸ðŸ¯²',
        '3Æ·ÈœÐ—â‘¢â¸â’ŠÓ â³Œêªêž«ï¼“âžŒð‘£Šð–¼»ðˆ†ðŸ‘ðŸ›ðŸ¥ðŸ¯ðŸ¹ðŸ¯³',
        '4áŽâ‘£â’‹ï¼”â¹âžð‘¢¯ðŸ’ðŸœðŸ¦ðŸ°ðŸºðŸ¯´',
        '5Æ¼â‘¤â’Œâºï¼•âžŽð‘¢»ðŸ“ðŸðŸ§ðŸ±ðŸ»ðŸ¯µ',
        '6â‘¥Ð±á®â»â’âžâ»ï¼–â³’ï¼–ð‘£•ðŸ”ðŸžðŸ¨ðŸ²ðŸ¼ðŸ¯¶',
        '7â‘¦ï¼—â¼â’Žâžð“’ð‘£†ðˆ’ðŸ•ðŸŸðŸ©ðŸ³ðŸ½ðŸ¯·',
        '8È¢È£â‘§â’ï¼˜à§ªâ½âž‘à©ªà¬ƒï¼˜ðŒšðŸ–ðŸ ðŸªðŸ´ðŸ¾ðž£‹ðŸ¯¸',
        '9à§­à©§â‘¨à­¨â’àµ­â³Šê®âž’ï¼™ð‘¢¬ð‘£Œâ¾ð‘£–ðŸ—ðŸ¡ðŸ«ðŸµðŸ¿ðŸ¯¹'
    ]
    cuda_available = torch.cuda.is_available()

    args = ClassificationArgs(eval_batch_size = 32, use_multiprocessing_for_evaluation=False)
    model = ClassificationModel('roberta', classify_model_path, num_labels=len(labels_list), use_cuda=cuda_available, args = args)

    wechat_terms = []
    predictions, raw_outputs = model.predict(terms_all_list)

    for index, term in enumerate(terms_all_list):
        if labels_list[predictions[index]] == 'wechat':
            wechat_terms.append(term)

    print(f'Finish contact classification, get wechat term {len(wechat_terms)}')
    #-------------------------------------------------------------------------------------------------
    sentences = []
    sentences_raw = []
    for term in wechat_terms:
        term_raw = term
        for char in term:
            for simple_num, similar_num in enumerate(unicode_similar_nums):
                if char in similar_num:
                    term = term.replace(char, str(simple_num))
                    break
        term = term.replace('â‘©', '10')
        term = term.replace('â’‘', '10')
        term = term.replace('â’’', '11')
        term = term.replace('â’“', '12')
        term = term.replace('â’”', '13')
        term = term.replace('â’•', '14')
        term = term.replace('â’–', '15')
        term = term.replace('â’—', '16')
        term = term.replace('â’˜', '17')
        term = term.replace('â’™', '18')
        term = term.replace('â’š', '19')
        term = term.replace('â’›', '20')
        term = term.replace('.', '')
        seg = jieba.cut(term)
        sentence = ''
        for s in seg:
            sentence += s + ' '
        # Token indices sequence length is longer than the specified maximum sequence length for this model (683 > 512). 
        # Running this sequence through the model will result in indexing errors
        if len(sentence) < 512:
            sentences_raw.append(term_raw)
            sentences.append(sentence)


    # Create a NERModel
    labels = ['B-contact', 'I-contact', 'O']

    cuda_available = torch.cuda.is_available()
    args = NERArgs(use_multiprocessing_for_evaluation=False)
    model = NERModel(
        "roberta",
        NER_model_path,
        labels=labels,
        use_cuda=cuda_available,
        args=args
    )

    # # Predictions on arbitary text strings
    predictions, raw_outputs = model.predict(sentences)
    
    for n, (preds, outs) in enumerate(zip(predictions, raw_outputs)):
        result = ['', '', '', '', '']
        result[0] = sentences_raw[n]
        flag = 0
        result_index = 1
        try:
            for pred, out in zip(preds, outs):
                key = list(pred.keys())[0]
                if flag == 0:
                    if pred[key] == 'B-contact':
                        flag = 1
                        result[result_index] += key
                elif flag == 1:
                    if pred[key] == 'I-contact':
                        flag = 2
                        result[result_index] += key
                    elif pred[key] == 'O':
                        result_index += 1
                        flag = 0
                    elif pred[key] == 'B-contact':
                        result_index += 1
                        flag = 1
                        result[result_index] += key
                elif flag == 2:
                    if pred[key] == 'I-contact':
                        flag = 2
                        result[result_index] += key
                    elif pred[key] == 'O':
                        result_index += 1
                        flag = 0
                    elif pred[key] == 'B-contact':
                        result_index += 1
                        flag = 1
                        result[result_index] += key
        except:
            print("\n___________________________")
            print("Sentence: ", sentences_raw[n])
            for pred, out in zip(preds, outs):
                key = list(pred.keys())[0]
                new_out = out[key]
                preds = list(softmax(np.mean(new_out, axis=0)))
                print(key, pred[key], preds[np.argmax(preds)], preds)
        if result[1] != '' and len(result[1]) > 4:
            result[1] = postprocess_wechat(result[1])
            if result[1] not in wechat_contact.keys():
                wechat_contact[result[1]] = [sentences_raw[n], 1]
                terms_with_wechat.add(sentences_raw[n])
                terms_extracted.add((sentences_raw[n], result[1]))
            else:
                wechat_contact[result[1]][1] += 1
                terms_with_wechat.add(sentences_raw[n])
                terms_extracted.add((sentences_raw[n], result[1]))


data = []

with open('data/Google/positive_data_predicted_fromcontact.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Google/positive_data_predicted_fromsite.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Bing/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Baidu/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Sogou/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

terms_all = set()
for item in data:
    term, link, kwd, timestamp, pagenum = eval(item)
    terms_all.add(term)

print(f'Finish Getting terms, get {len(terms_all)} terms')
data = []
#-------------------------------------------------------------------------------------------------
labels_list = ['website', 'wechat', 'qq', 'telegram', 'others','telephone']
labels_dict = {'website':0, 'wechat':1, 'qq':2, 'telegram':3, 'others':4, 'telephone':5}
terms_all_list = []
wechat_contact = {}
num = 0
index_term = 0

for item in terms_all:
    num += 1
    index_term += 1
    terms_all_list.append(item)
    if num >= 50000:
        print(f'Get {len(terms_all_list)} positive terms')
        get_wechat_contact(terms_all_list, wechat_contact)
        logging.info(f'Finished terms {index_term} of {len(terms_all)} for extracting wechat contact, get {len(wechat_contact)} wechat contacts')
        num = 0
        terms_all_list = []
        with open('data/wechat_contact.txt', 'w', encoding='utf-8') as fp:
            wechat_list = []
            for wechat in wechat_contact:
                wechat_list.append((wechat, wechat_contact[wechat][0], wechat_contact[wechat][1]))

            wechat_list.sort(key=lambda x: -x[2])
            for item in wechat_list:
                fp.write(str(item))
                fp.write('\n')


print(f'Get {len(terms_all_list)} positive terms')

get_wechat_contact(terms_all_list, wechat_contact)

print(f'Finish extract wechat contact, get {len(wechat_contact)}')
with open('data/wechat_contact.txt', 'w', encoding='utf-8') as fp:
    wechat_list = []
    for wechat in wechat_contact:
        wechat_list.append((wechat, wechat_contact[wechat][0], wechat_contact[wechat][1]))

    wechat_list.sort(key=lambda x: -x[2])
    for item in wechat_list:
        fp.write(str(item))
        fp.write('\n')

with open('data/terms_with_wechat.txt', 'w', encoding='utf-8') as fp:
    for item in terms_with_wechat:
        fp.write(item)
        fp.write('\n')

with open('data/terms_extracted_wechat.txt', 'w', encoding='utf-8') as fp:
    for item in terms_extracted:
        fp.write(str(item))
        fp.write('\n')