from simpletransformers.classification import ClassificationModel, ClassificationArgs
import numpy as np
import pandas as pd
from scipy.special import softmax
import csv
from simpletransformers.ner import NERModel, NERArgs
import jieba
import logging
from sklearn.metrics import f1_score,  accuracy_score
from sklearn.model_selection import train_test_split
import torch
import re

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

NER_model_path = '/data/jlxue/RBSEO_NER_train_model'
classify_model_path = '/data/jlxue/RBSEO_contact_classifier_train_model'

terms_with_qq = set()

def get_qq_contact(terms_all_list, qq_contact):
    unicode_similar_nums = [
        '0OoÎŸÃ’Î¿ÏƒÐžÐ¾ï¼Õ•Ã˜Î˜Ö…×¡Ù‡Ù¥Ú¾ÛÛ•Ûµß€à¥¦à§¦à©¦à«¦à¬ à­¦à¯¦à°‚à±¦à²‚à³¦à´‚à´ àµ¦à¶‚à¹à»á€á€áƒ¿á‹á´á´‘â„´â²žâ²Ÿâµ”ã€‡ê“³ê¬½ï®¦ï®§ï®¨ï®©ï®ªï®«ï®¬ï®­ï»©ï»ªï»«ï»¬ï¼ï¼¯ï½ðŠ’ðŠ«ð„ð¬ð“‚ð“ªð”–ð‘“ð‘¢µð‘£ˆð‘£—ð‘£ ðŽð¨ð‘‚ð‘œð‘¶ð’ð’ªð“žð“¸ð”’ð”¬ð•†ð• ð•ºð–”ð–®ð—ˆð—¢ð—¼ð˜–ð˜°ð™Šð™¤ð™¾ðš˜ðš¶ð›ð›”ð›°ðœŠðœŽðœªð„ðˆð¤ð¾ðž‚ðžžðž¸ðž¼ðŸŽðŸ˜ðŸ¢ðŸ¬ðŸ¶ðž¸¤ðž¹¤ðžº„ðŸ¯°',
        '1Iil|Ä±â’ˆÆ–â‘ ï¼‘Ç€É©ÉªË›ÍºÎ™Î¹Ð†Ñ–Ó€Ó×€×•×ŸØ§Ù¡Û±ßŠáŽ¥á›á¾¾â„â„‘â„“â„¹â…ˆâ… â…°â…¼âˆ£â³â½â²’âµê“²ê™‡ê­µïºïºŽï¼‘ï¼©ï½‰ï½Œï¿¨ðŠŠðŒ‰ðŒ ð‘£ƒð–¼¨ðˆð¢ð¥ð¼ð‘–ð‘™ð‘°ð’Šð’ð’¾ð“ð“˜ð“²ð“µð”¦ð”©ð•€ð•šð•ð•´ð–Žð–‘ð–¨ð—‚ð—…ð—œð—¶ð—¹ð˜ð˜ªð˜­ð™„ð™žð™¡ð™¸ðš’ðš•ðš¤ðš°ð›Šð›ªðœ„ðœ¤ðœ¾ðžð¸ðž˜ðž²ðŸðŸ™ðŸ£ðŸ­ðŸ·ðž£‡ðž¸€ðžº€ðŸ¯±',
        '2Æ§Ï¨â’‰á’¿ê™„â‘¡ê›¯êšï¼’ðŸðŸšðŸ¤ðŸ®ðŸ¸ðŸ¯²',
        '3Æ·ÈœÐ—â‘¢â’ŠÓ â³Œêªêž«ï¼“ð‘£Šð–¼»ðˆ†ðŸ‘ðŸ›ðŸ¥ðŸ¯ðŸ¹ðŸ¯³',
        '4áŽâ‘£â’‹ï¼”ð‘¢¯ðŸ’ðŸœðŸ¦ðŸ°ðŸºðŸ¯´',
        '5Æ¼â‘¤â’Œï¼•ð‘¢»ðŸ“ðŸðŸ§ðŸ±ðŸ»ðŸ¯µ',
        '6â‘¥Ð±á®â’ï¼–â³’ï¼–ð‘£•ðŸ”ðŸžðŸ¨ðŸ²ðŸ¼ðŸ¯¶',
        '7â‘¦ï¼—â’Žð“’ð‘£†ðˆ’ðŸ•ðŸŸðŸ©ðŸ³ðŸ½ðŸ¯·',
        '8È¢È£â‘§â’ï¼˜à§ªà©ªà¬ƒï¼˜ðŒšðŸ–ðŸ ðŸªðŸ´ðŸ¾ðž£‹ðŸ¯¸',
        '9à§­à©§â‘¨à­¨â’àµ­â³Šê®ï¼™ð‘¢¬ð‘£Œð‘£–ðŸ—ðŸ¡ðŸ«ðŸµðŸ¿ðŸ¯¹'
    ]
    cuda_available = torch.cuda.is_available()

    args = ClassificationArgs(eval_batch_size = 32, use_multiprocessing_for_evaluation=False)
    model = ClassificationModel('roberta', classify_model_path, num_labels=len(labels_list), use_cuda=cuda_available, args = args)

    qq_terms = []
    predictions, raw_outputs = model.predict(terms_all_list)

    for index, term in enumerate(terms_all_list):
        if labels_list[predictions[index]] == 'qq':
            qq_terms.append(term)

    print(f'Finish contact classification, get qq term {len(qq_terms)}')

    for term in qq_terms:
        term_origin = term
        term = term.lower()
        term = term.replace('q', '')
        term = term.replace('-', '')
        term = term.replace('_', '')
        term = term.replace('â€”', '')
        term = term.replace('æ‰£', '')
        term = term.replace('â’‘', '10')
        term = term.replace('â’’', '11')
        term = term.replace('â’“', '12')
        term = term.replace('â’”', '13')
        term = term.replace('â’•', '14')
        term = term.replace('â’–', '15')
        term = term.replace('â’—', '16')
        term = term.replace('â’˜', '17')
        term = term.replace('â’™', '18')
        term = term.replace('â’š', '19')
        term = term.replace('â’›', '20')
        term = term.replace('.', '')
        term = term.replace(' ', '')
        for char in term:
            for simple_num, similar_num in enumerate(unicode_similar_nums):
                if char in similar_num:
                    term = term.replace(char, str(simple_num))
                    break

        compileX = re.compile(r"\d+")
        num_result = compileX.findall(term)
        for nums in num_result:
            if len(nums) >= 7 and len(nums) <= 12:
                if nums not in qq_contact.keys():
                    qq_contact[nums] = [term_origin, 1]
                    terms_with_qq.add(term_origin)
                else:
                    qq_contact[nums][1] += 1
                    terms_with_qq.add(term_origin)
                break
    #-------------------------------------------------------------------------------------------------
    


data = []

with open('data/Google/positive_data_predicted_fromcontact.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Google/positive_data_predicted_fromsite.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Bing/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Baidu/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Sogou/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

terms_all = set()
for item in data:
    term, link, kwd, timestamp, pagenum = eval(item)
    terms_all.add(term)

print(f'Finish Getting terms, get {len(terms_all)} terms')

#-------------------------------------------------------------------------------------------------
labels_list = ['website', 'wechat', 'qq', 'telegram', 'others','telephone']
labels_dict = {'website':0, 'wechat':1, 'qq':2, 'telegram':3, 'others':4, 'telephone':5}
terms_all_list = []
qq_contact = {}
num = 0
index_term = 0

for item in terms_all:
    num += 1
    index_term += 1
    terms_all_list.append(item)
    if num >= 50000:
        print(f'Get {len(terms_all_list)} positive terms')
        get_qq_contact(terms_all_list, qq_contact)
        logging.info(f'Finished terms {index_term} of {len(terms_all)} for extracting qq contact, get {len(qq_contact)} qq contacts')
        num = 0
        terms_all_list = []


print(f'Get {len(terms_all_list)} positive terms')

get_qq_contact(terms_all_list, qq_contact)

print(f'Finish extract qq contact, get {len(qq_contact)}')
with open('data/qq_contact.txt', 'w', encoding='utf-8') as fp:
    qq_list = []
    for qq in qq_contact:
        qq_list.append((qq, qq_contact[qq][0], qq_contact[qq][1]))

    qq_list.sort(key=lambda x: -x[2])
    for item in qq_list:
        fp.write(str(item))
        fp.write('\n')

with open('data/terms_with_qq.txt', 'w', encoding='utf-8') as fp:
    for item in terms_with_qq:
        fp.write(item)
        fp.write('\n')