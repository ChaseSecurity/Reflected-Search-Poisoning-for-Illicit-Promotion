from simpletransformers.classification import ClassificationModel, ClassificationArgs
import numpy as np
import pandas as pd
from scipy.special import softmax
import csv
from simpletransformers.ner import NERModel, NERArgs
import jieba
import logging
from sklearn.metrics import f1_score,  accuracy_score
from sklearn.model_selection import train_test_split
import torch
import re

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

NER_model_path = '/data/jlxue/RBSEO_NER_train_model'
classify_model_path = '/data/jlxue/RBSEO_contact_classifier_train_model'
qq_succeed_path = '/data/jlxue/qq_succeed_classifier_model'


def get_qq_contact(terms_all_list, qq_contact):
    unicode_similar_nums = [
        '0OoÎŸÎ¿ÏƒÐžÐ¾Õ•Ö…×¡Ù‡Ù¥Ú¾ÛÛ•Ûµß€à¥¦à§¦à©¦à«¦à¬ à­¦à¯¦à°‚à±¦à²‚à³¦à´‚à´ àµ¦à¶‚à¹à»á€á€áƒ¿á‹á´á´‘â„´â²žâ²Ÿâµ”ã€‡ê“³ê¬½ï®¦ï®§ï®¨ï®©ï®ªï®«ï®¬ï®­ï»©ï»ªï»«ï»¬ï¼ï¼¯ï½ðŠ’ðŠ«ð„ð¬ð“‚ð“ªð”–ð‘“ð‘¢µð‘£ˆð‘£—ð‘£ ðŽð¨ð‘‚ð‘œð‘¶ð’ð’ªð“žð“¸ð”’ð”¬ð•†ð• ð•ºð–”ð–®ð—ˆð—¢ð—¼ð˜–ð˜°ð™Šð™¤ð™¾ðš˜ðš¶ð›ð›”ð›°ðœŠðœŽðœªð„ðˆð¤ð¾ðž‚ðžžðž¸ðž¼ðŸŽðŸ˜ðŸ¢ðŸ¬ðŸ¶ðž¸¤ðž¹¤ðžº„ðŸ¯°',
        '1Iil|Ä±Æ–Ç€É©ÉªË›ÍºÎ™Î¹Ð†Ñ–Ó€Ó×€×•×ŸØ§Ù¡Û±ßŠáŽ¥á›á¾¾â„â„‘â„“â„¹â…ˆâ… â…°â…¼âˆ£â³â½â²’âµê“²ê™‡ê­µïºïºŽï¼‘ï¼©ï½‰ï½Œï¿¨ðŠŠðŒ‰ðŒ ð‘£ƒð–¼¨ðˆð¢ð¥ð¼ð‘–ð‘™ð‘°ð’Šð’ð’¾ð“ð“˜ð“²ð“µð”¦ð”©ð•€ð•šð•ð•´ð–Žð–‘ð–¨ð—‚ð—…ð—œð—¶ð—¹ð˜ð˜ªð˜­ð™„ð™žð™¡ð™¸ðš’ðš•ðš¤ðš°ð›Šð›ªðœ„ðœ¤ðœ¾ðžð¸ðž˜ðž²ðŸðŸ™ðŸ£ðŸ­ðŸ·ðž£‡ðž¸€ðžº€ðŸ¯±',
        '2Æ§Ï¨á’¿ê™„ê›¯êšï¼’ðŸðŸšðŸ¤ðŸ®ðŸ¸ðŸ¯²',
        '3Æ·ÈœÐ—Ó â³Œêªêž«ï¼“ð‘£Šð–¼»ðˆ†ðŸ‘ðŸ›ðŸ¥ðŸ¯ðŸ¹ðŸ¯³',
        '4áŽï¼”ð‘¢¯ðŸ’ðŸœðŸ¦ðŸ°ðŸºðŸ¯´',
        '5Æ¼ï¼•ð‘¢»ðŸ“ðŸðŸ§ðŸ±ðŸ»ðŸ¯µ',
        '6Ð±á®â³’ï¼–ð‘£•ðŸ”ðŸžðŸ¨ðŸ²ðŸ¼ðŸ¯¶',
        '7ï¼—ð“’ð‘£†ðˆ’ðŸ•ðŸŸðŸ©ðŸ³ðŸ½ðŸ¯·',
        '8È¢È£à§ªà©ªà¬ƒï¼˜ðŒšðŸ–ðŸ ðŸªðŸ´ðŸ¾ðž£‹ðŸ¯¸',
        '9à§­à©§à­¨àµ­â³Šê®ï¼™ð‘¢¬ð‘£Œð‘£–ðŸ—ðŸ¡ðŸ«ðŸµðŸ¿ðŸ¯¹'
    ]
    cuda_available = torch.cuda.is_available()

    args = ClassificationArgs(eval_batch_size = 32, use_multiprocessing_for_evaluation=False)
    model = ClassificationModel('roberta', classify_model_path, num_labels=len(labels_list), use_cuda=cuda_available, args = args)

    qq_terms_origin = []
    predictions, raw_outputs = model.predict(terms_all_list)

    for index, term in enumerate(terms_all_list):
        if labels_list[predictions[index]] == 'qq':
            qq_terms_origin.append(term)

    qq_terms = []
    args = ClassificationArgs(eval_batch_size = 32, use_multiprocessing_for_evaluation=False)
    model = ClassificationModel('roberta', qq_succeed_path, use_cuda=cuda_available, args = args)
    predictions, raw_outputs = model.predict(qq_terms_origin)
    for index, term in enumerate(qq_terms_origin):
        if predictions[index] == 1:
            qq_terms.append(term)

    print(f'Finish contact classification, get qq term {len(qq_terms)}')

    for term in qq_terms:
        term_origin = term
        term = term.lower()
        term = term.replace('q', '')
        term = term.replace('-', '')
        term = term.replace('_', '')
        term = term.replace('â€”', '')
        term = term.replace('æ‰£', '')
        for char in term:
            for simple_num, similar_num in enumerate(unicode_similar_nums):
                if char in similar_num:
                    term = term.replace(char, str(simple_num))
                    break

        compileX = re.compile(r"\d+")
        num_result = compileX.findall(term)
        for nums in num_result:
            if len(nums) >= 7 and len(nums) <= 12:
                qq_contact[nums] = term_origin
                break
    #-------------------------------------------------------------------------------------------------
    


data = []

with open('data/Google/positive_data_predicted_fromcontact.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Google/positive_data_predicted_fromsite.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Bing/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Baidu/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

with open('data/Sogou/positive_data_predicted.txt', 'r', encoding = 'utf-8') as fp:
    data += fp.readlines()

terms_all = set()
for item in data:
    term, link, kwd, timestamp, pagenum = eval(item)
    terms_all.add(term)

print(f'Finish Getting terms, get {len(terms_all)} terms')

#-------------------------------------------------------------------------------------------------
labels_list = ['website', 'wechat', 'qq', 'telegram', 'others']
labels_dict = {'website':0, 'wechat':1, 'qq':2, 'telegram':3, 'others':4}
terms_all_list = []
qq_contact = {}
num = 0
index_term = 0

for item in terms_all:
    num += 1
    index_term += 1
    terms_all_list.append(item)
    if num >= 50000:
        print(f'Get {len(terms_all_list)} positive terms')
        get_qq_contact(terms_all_list, qq_contact)
        logging.info(f'Finished terms {index_term} of {len(terms_all)} for extracting qq contact, get {len(qq_contact)} qq contacts')
        num = 0
        terms_all_list = []


print(f'Get {len(terms_all_list)} positive terms')

get_qq_contact(terms_all_list, qq_contact)

print(f'Finish extract qq contact, get {len(qq_contact)}')
with open('data/qq_contact.txt', 'w', encoding='utf-8') as fp:
    for item in qq_contact:
        fp.write(str((item, qq_contact[item])))
        fp.write('\n')

